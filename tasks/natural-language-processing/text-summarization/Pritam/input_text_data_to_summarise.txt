The shrinking phase of MorphNet is useful to produce a neural network that optimizes the cost for a specific resource. However, that optimization could come at the cost of accuracy. That's precisesly why MorphNet uses an expanding phase based on a width multiplier to expand the sizes of all layers. For example, an expansion of 50% will cause inefficient layer that started with 100 neurons and shrank to 10 would only expand back to 15, while an important layer that only shrank to 80 neurons might expand to 120 and have more resources with which to work. The net effect is re-allocation of computational resources from less efficient parts of the network to parts of the network where they might be more useful.